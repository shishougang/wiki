<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<title>Data Structures&amp;Algorithm Analysis in C++(Second Edition)</title>
<!-- 2016-05-26 Thu 22:15 -->
<meta  http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta  name="generator" content="Org-mode" />
<meta  name="author" content="Shi Shougang" />
<link href="../../assets/bootstrap.min.css" rel="stylesheet" media="screen">
<link href="../../assets/bootstrap-responsive.min.css" rel="stylesheet">
<link rel="stylesheet" type="text/css" href="../../assets/stylesheet.css" />
<script src="assets/js/bootstrap.min.js"></script>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/javascript" src="http://orgmode.org/mathjax/MathJax.js"></script>
<script type="text/javascript">
<!--/*--><![CDATA[/*><!--*/
    MathJax.Hub.Config({
        // Only one of the two following lines, depending on user settings
        // First allows browser-native MathML display, second forces HTML/CSS
        //  config: ["MMLorHTML.js"], jax: ["input/TeX"],
            jax: ["input/TeX", "output/HTML-CSS"],
        extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js",
                     "TeX/noUndefined.js"],
        tex2jax: {
            inlineMath: [ ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"], ["\\begin{displaymath}","\\end{displaymath}"] ],
            skipTags: ["script","noscript","style","textarea","pre","code"],
            ignoreClass: "tex2jax_ignore",
            processEscapes: false,
            processEnvironments: true,
            preview: "TeX"
        },
        showProcessingMessages: true,
        displayAlign: "center",
        displayIndent: "2em",

        "HTML-CSS": {
             scale: 100,
             availableFonts: ["STIX","TeX"],
             preferredFont: "TeX",
             webFont: "TeX",
             imageFont: "TeX",
             showMathMenu: true,
        },
        MMLorHTML: {
             prefer: {
                 MSIE:    "MML",
                 Firefox: "MML",
                 Opera:   "HTML",
                 other:   "HTML"
             }
        }
    });
/*]]>*///-->
</script>
</head>
<body>
<div id="content">
<h1 class="title">Data Structures&amp;Algorithm Analysis in C++(Second Edition)</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#sec-1">Sites</a>
<ul>
<li><a href="#sec-1-1">FTP</a></li>
<li><a href="#sec-1-2">author homepage</a></li>
<li><a href="#sec-1-3">source codes</a></li>
</ul>
</li>
<li><a href="#sec-2">References</a>
<ul>
<li><a href="#sec-2-1">Chapter 1</a></li>
<li><a href="#sec-2-2">Chapter 2</a></li>
<li><a href="#sec-2-3">Chapter 4: Trees</a></li>
<li><a href="#sec-2-4">CHAPTER 5: HASHING</a></li>
<li><a href="#sec-2-5">CHAPTER 6: PRIORITY QUEUES (HEAPS)</a></li>
<li><a href="#sec-2-6">CHAPTER 7: SORTING</a></li>
<li><a href="#sec-2-7">CHAPTER 8: THE DISJOINT SET ADT</a></li>
<li><a href="#sec-2-8">CHAPTER 9: GRAPH ALGORITHMS</a></li>
<li><a href="#sec-2-9">CHAPTER 10: ALGORITHM DESIGN TECHNIQUES</a></li>
<li><a href="#sec-2-10">CHAPTER 11: AMORTIZED ANALYSIS</a></li>
</ul>
</li>
<li><a href="#sec-3">Notes</a>
<ul>
<li><a href="#sec-3-1">Chapter 1: INTRODUCTION</a></li>
<li><a href="#sec-3-2">CHAPTER 2: ALGORITHM ANALYSIS</a></li>
<li><a href="#sec-3-3">Chapter 3: Lists, Stacks, and Queues</a></li>
<li><a href="#sec-3-4">Chapter 4: Trees</a></li>
<li><a href="#sec-3-5">CHAPTER 5: HASHING</a></li>
<li><a href="#sec-3-6">CHAPTER 6: PRIORITY QUEUES (HEAPS)</a></li>
<li><a href="#sec-3-7">CHAPTER 7: SORTING</a></li>
<li><a href="#sec-3-8">CHAPTER 8: THE DISJOINT SET ADT</a></li>
<li><a href="#sec-3-9">CHAPTER 9: GRAPH ALGORITHMS</a></li>
<li><a href="#sec-3-10">CHAPTER 10: ALGORITHM DESIGN TECHNIQUES</a></li>
<li><a href="#sec-3-11">CHAPTER 11: AMORTIZED ANALYSIS</a></li>
</ul>
</li>
</ul>
</div>
</div>


<div id="outline-container-sec-1" class="outline-2">
<h2 id="sec-1">Sites</h2>
<div class="outline-text-2" id="text-1">
</div><div id="outline-container-sec-1-1" class="outline-3">
<h3 id="sec-1-1">FTP</h3>
<div class="outline-text-3" id="text-1-1">
<p>
ftp.awl.com
</p>
</div>
</div>
<div id="outline-container-sec-1-2" class="outline-3">
<h3 id="sec-1-2">author homepage</h3>
<div class="outline-text-3" id="text-1-2">
<p>
<a href="http://users.cis.fiu.edu/~weiss/">http://users.cis.fiu.edu/~weiss/</a>
</p>
</div>
</div>
<div id="outline-container-sec-1-3" class="outline-3">
<h3 id="sec-1-3">source codes</h3>
<div class="outline-text-3" id="text-1-3">
<ul class="org-ul">
<li><a href="http://users.cis.fiu.edu/~weiss/dsaa_c++/code/">http://users.cis.fiu.edu/~weiss/dsaa_c++/code/</a>
</li>
<li><a href="http://users.cis.fiu.edu/~weiss/dsaa_c++/code/g++.txt">http://users.cis.fiu.edu/~weiss/dsaa_c++/code/g++.txt</a>
</li>
<li><a href="http://users.cis.fiu.edu/~weiss/adspc++2/code/">http://users.cis.fiu.edu/~weiss/adspc++2/code/</a>
</li>
</ul>
</div>
<ul class="org-ul"><li><a id="sec-1-3-1" name="sec-1-3-1"></a>codes review<br  /><div class="outline-text-4" id="text-1-3-1">
<p>
Global Utilities
</p>

<p>
StartConv.h 
EndConv.h 
getline.cpp
Chapter 1: Arrays, Pointers, Structures
</p>

<p>
ArrayDemo.cpp 
GetInts.cpp 
TestString.cpp 
TestSwap.cpp
Chapter 2: Classes
</p>

<p>
IntCell.h  IntCell.cpp  TestIntCell.cpp 
BuggyIntCell.cpp 
DeepIntCell.cpp 
Rational.h  Rational.cpp  RatMain.cpp 
mystring.h  string.cpp
Chapter 3: Templates
</p>

<p>
FuncTemplates.cpp 
InsSort.cpp 
MemoryCell.h  MemoryCell.cpp  TestMemoryCell.cpp 
Matrix.h 
vector.h  vector.cpp
Chapter 4: Inheritance
</p>

<p>
Except.h 
Shape.cpp  
Hiding.cpp 
StaticBinding.cpp
Chapter 5: Patterns
</p>

<p>
Rectangle.cpp 
Wrapper.h 
Ambiguity.cpp 
AutoPtr.cpp 
StorageCell.h TestStorageCell.cpp 
Iterator1.cpp 
Iterator2.cpp 
Iterator3.cpp 
pair.h 
Observer.cpp
Chapter 6: Running Times
</p>

<p>
MaxSum.cpp 
BinarySearch.cpp
Chapter 7: STL
</p>

<p>
vector.h  vector.cpp 
functional.h TestFunctional.cpp 
algorithm.h 
SimpleSetDemo.cpp 
TestPQ.cpp
Chapter 8: Recursion
</p>

<p>
RecSum.cpp 
PrintInt.cpp 
BinarySearchRec.cpp 
Ruler.java 
FractalStar.java 
Math.cpp 
MaxSum.cpp 
MkChnge.cpp 
TicTacSlow.cpp
Chapter 9: Sorting
</p>

<p>
Duplicate.cpp 
Sort.h  TestSort.cpp
Chapter 10: Randomization
</p>

<p>
Random.h  Random.cpp  RandTest.cpp 
Permute.cpp 
Math.cpp
Chapter 11: Fun and Games
</p>

<p>
WordSrch.cpp 
TicTac.cpp
Chapter 12: Applications of Stacks &#x2013; Compilers and Parsing
</p>

<p>
Tokenizer.h  Tokenizer.cpp  Balance.cpp 
Infix.cpp
Chapter 13: Utilities
</p>

<p>
Hzip.cpp 
Tokenizer.h  Tokenizer.cpp  Xref.cpp
Chapter 14: Simulation
</p>

<p>
Josephus.cpp 
Modems.cpp
Chapter 15: Shortest Path Algorithms
</p>

<p>
Paths.cpp Note for DotNet users: on line 40 of PairingHeap.cpp, insert the word typename in front of the line so it appears as typename PairingHeap::Position
Chapter 16: Stacks
</p>

<p>
StackAr.h  StackAr.cpp  TestStackAr.cpp 
StackLi.h  StackLi.cpp  TestStackLi.cpp 
QueueAr.h  QueueAr.cpp  TestQueueAr.cpp 
QueueLi.h  QueueLi.cpp  TestQueueLi.cpp
Chapter 17: Linked Lists
</p>

<p>
LinkedList.h  LinkedList.cpp  SortLinkedList.h  SortLinkedList.cpp  TestLinkedList.cpp 
list.h  list.cpp  TestList.cpp
Chapter 18: Trees
</p>

<p>
BinaryTree.h  BinaryTree.cpp 
Iterate.h  Iterate.cpp  TestBinaryTree.cpp
Chapter 19: Search Trees
</p>

<p>
BinarySearchTree.h  BinarySearchTree.cpp  TestBinarySearchTree.cpp 
RedBlackTree.h  RedBlackTree.cpp  TestRedBlackTree.cpp 
AATree.h  AATree.cpp  TestAATree.cpp 
set.h  set.cpp  TestSet.cpp 
map.h  map.cpp  TestMap.cpp
Chapter 20: Hash Tables
</p>

<p>
QuadraticProbing.h  QuadraticProbing.cpp  TestQuadraticProbing.cpp
Chapter 21: Heaps
</p>

<p>
BinaryHeap.h  BinaryHeap.cpp  TestBinaryHeap.cpp 
queue.h  queue.cpp  TestQueue.cpp
Chapter 22: Splay Trees
</p>

<p>
SplayTree.h  SplayTree.cpp  TestSplayTree.cpp
Chapter 23: Pairing Heaps
</p>

<p>
PairingHeap.h  PairingHeap.cpp  TestPairingHeap.cpp Note for DotNet users: on line 40 of PairingHeap.cpp, insert the word typename in front of the line so it appears as typename PairingHeap::Position
Chapter 24: Disjoint Sets
</p>

<p>
DisjSets.h  DisjSets.cpp  TestDisjSets.cpp
Appendix D: Primitive Arrays
</p>

<p>
PointerHopping.cpp
</p>
</div>
</li></ul>
</div>
</div>

<div id="outline-container-sec-2" class="outline-2">
<h2 id="sec-2">References</h2>
<div class="outline-text-2" id="text-2">
</div><div id="outline-container-sec-2-1" class="outline-3">
<h3 id="sec-2-1">Chapter 1</h3>
<div class="outline-text-3" id="text-2-1">
<p>
There are many good textbooks covering the mathematics reviewed in this chapter. A small subset
is <code>[1], [2], [3], [11], [13], and [14].</code> Reference <code>[11]</code> is specifically geared toward the analysis
of algorithms. It is the first volume of a three-volume series that will be cited throughout this
text. More advanced material is covered in <code>[6]</code>.
Throughout this book we will assume a knowledge of C <code>[10]</code>. Occasionally, we add a feature where
necessary for clarity. We also assume familiarity with pointers and recursion (the recursion
summary in this chapter is meant to be a quick review). We will attempt to provide hints on their
use where appropriate throughout the textbook. Readers not familiar with these should consult
<code>[4], [8], [12]</code>, or any good intermediate programming textbook.
</p>

<p>
General programming style is discussed in several books. Some of the classics are <code>[5], [7]</code>, and
<code>[9]</code>.
</p>

<ol class="org-ol">
<li>M. O. Albertson and J. P. Hutchinson, Discrete Mathematics with Algorithms, John Wiley &amp; Sons,
New York, 1988.
</li>
<li>Z. Bavel, Math Companion for Computer Science, Reston Publishing Company, Reston, Va., 1982.
</li>
<li>R. A. Brualdi, Introductory Combinatorics, North-Holland, New York, 1977.
</li>
<li>W. H. Burge, Recursive Programming Techniques, Addison-Wesley, Reading, Mass., 1975.
</li>
<li>E. W. Dijkstra, A Discipline of Programming, Prentice Hall, Englewood Cliffs, N.J., 1976.
</li>
<li>R. L. Graham, D. E. Knuth, and O. Patashnik, Concrete Mathematics, Addison-Wesley, Reading,
Mass., 1989.
</li>
<li>D. Gries, The Science of Programming, Springer-Verlag, New York, 1981.
</li>
<li>P. Helman and R. Veroff, Walls and Mirrors: Intermediate Problem Solving and Data Structures,
2d ed., Benjamin Cummings Publishing, Menlo Park, Calif., 1988.
</li>
<li>B. W. Kernighan and P. J. Plauger, The Elements of Programming Style, 2d ed., McGraw- Hill,
New York, 1978.
</li>
<li>B. W. Kernighan and D. M. Ritchie, The C Programming Language, 2d ed., Prentice Hall,
Englewood Cliffs, N.J., 1988.
</li>
<li>D. E. Knuth, The Art of Computer Programming, Vol. 1: Fundamental Algorithms, 2d ed.,
Addison-Wesley, Reading, Mass., 1973.
</li>
<li>E. Roberts, Thinking Recursively, John Wiley &amp; Sons, New York, 1986.
</li>
<li>F. S. Roberts, Applied Combinatorics, Prentice Hall, Englewood Cliffs, N.J., 1984.
</li>
<li>A. Tucker, Applied Combinatorics, 2d ed., John Wiley &amp; Sons, New York, 1984.
</li>
</ol>
</div>
</div>
<div id="outline-container-sec-2-2" class="outline-3">
<h3 id="sec-2-2">Chapter 2</h3>
<div class="outline-text-3" id="text-2-2">
<ol class="org-ol">
<li>A. V. Aho, J. E. Hopcroft, and J. D. Ullman, The Design and
Analysis of Computer Algorithms, Addison-Wesley, Reading, Mass., 1974.
</li>
<li>J. L. Bentley, Writing Efficient Programs, Prentice Hall, Englewood Cliffs, N.J., 1982.
</li>
<li>J. L. Bentley, Programming Pearls, Addison-Wesley, Reading, Mass., 1986.
</li>
<li>J. L. Bentley, More Programming Pearls, Addison-Wesley, Reading, Mass., 1988.
</li>
<li>D. E. Knuth, The Art of Computer Programming, Vol 1: Fundamental Algorithms, 2d ed., Addison-Wesley, Reading, Mass., 1973.
</li>
<li>D. E. Knuth, The Art of Computer Programming, Vol 2: Seminumerical Algorithms, 2d ed., Addison-Wesley, Reading, Mass., 1981.
</li>
<li>D. E. Knuth, The Art of Computer Programming, Vol 3: Sorting and Searching, Addison-Wesley, Reading, Mass., 1975.
</li>
<li>D. E. Knuth, "Big Omicron and Big Omega and Big Theta," ACM SIGACT News, 8 (1976), 18-23.
</li>
</ol>
</div>
</div>

<div id="outline-container-sec-2-3" class="outline-3">
<h3 id="sec-2-3">Chapter 4: Trees</h3>
<div class="outline-text-3" id="text-2-3">
More information on binary search trees, and in particular the mathematical
properties of trees can be found in the two books by Knuth [23] and [24].
Several papers deal with the lack of balance caused by biased deletion algorithms
in binary search trees. Hibbard's paper [20] proposed the original deletion
algorithm and established that one deletion preserves the randomness of the
trees. A complete analysis has been performed only for trees with three [21] and
four nodes[5]. Eppinger's paper [15] provided early empirical evidence of
nonrandomness, and the papers by Culberson and Munro, [11], [12], provide some
analytical evidence (but not a complete proof for the general case of intermixed
insertions and deletions).<br>

AVL trees were proposed by Adelson-Velskii and Landis [1]. Simulation results for
AVL trees, and variants in which the height imbalance is allowed to be at most
k for various values of k, are presented in [22]. A deletion algorithm for AVL
trees can be found in [24]. Analysis of the averaged depth of AVL trees is
incomplete, but some results are contained in [25].
[3] and [9] considered self-adjusting trees like the type in Section 4.5.1. Splay
trees are described in [29].<br>

B-trees first appeared in [6]. The implementation described in the original paper
allows data to be stored in internal nodes as well as leaves. The data structure
we have described is sometimes known as a B+ tree. A survey of the different
types of B-trees is presented in [10]. Empirical results of the various schemes
is reported in [18]. Analysis of 2-3 trees and B-trees can be found in [4], [14],
and [33].<br>

Exercise 4.14 is deceptively difficult. A solution can be found in [16]. Exercise
4.26 is from [32]. Information on B*-trees, described in Exercise 4.38, can be
found in [13]. Exercise 4.42 is from [2]. A solution to Exercise 4.43 using 2n -6
rotations is given in [30]. Using threads, a la Exercise 4.45, was first proposed
in [28]. k-d trees were first proposed in [7]. Their major drawback is that both
deletion and balancing are difficult. [8] discusses k-d trees and other methods
used for multidimensional searching.<br>

Other popular balanced search trees are red-black trees [19] and weight-balanced
trees [27]. More balanced tree schemes can be found in the books [17], [26], and
[31].<br>
<ol class="org-ol">
<li>G. M. Adelson-Velskii and E. M. Landis, "An Algorithm for the
Organization of Information," Soviet Math. Doklady 3 (1962), 1259-1263.
</li>
<li>A. V. Aho, J. E. Hopcroft, and J. D. Ullman, The Design and
Analysis of Computer Algorithms, Addison-Wesley, Reading, MA, 1974.
</li>
<li>B. Allen and J. I. Munro, "Self Organizing Search Trees," Journal
of the ACM, 25 (1978), 526-535.
</li>
<li>R. A. Baeza-Yates, "Expected Behaviour of B+- trees under Random
Insertions," Acta Informatica 26 (1989), 439-471.
</li>
<li>R. A. Baeza-Yates, "A Trivial Algorithm Whose Analysis Isn't: A
Continuation," BIT 29 (1989), 88-113.
</li>
<li>R. Bayer and E. M. McGreight, "Organization and Maintenance of
Large Ordered Indices," Acta Informatica 1 (1972), 173-189.
</li>
<li>J. L. Bentley, "Multidimensional Binary Search Trees Used for
Associative Searching," Communications of the ACM 18 (1975), 509-517.
</li>
<li>J. L. Bentley and J. H. Friedman, "Data Structures for Range
Searching," Computing Surveys 11 (1979), 397-409.
</li>
<li>J. R. Bitner, "Heuristics that Dynamically Organize Data
Structures," SIAM Journal on Computing 8 (1979), 82-110.
</li>
<li>D. Comer, "The Ubiquitous B-tree," Computing Surveys 11 (1979), 121-137.
</li>
<li>J. Culberson and J. I. Munro, "Explaining the Behavior of Binary
Search Trees under Prolonged Updates: A Model and Simulations,"
Computer Journal 32 (1989), 68-75.
</li>
<li>J. Culberson and J. I. Munro, "Analysis of the Standard Deletion
Algorithms' in Exact Fit Domain Binary Search Trees," Algorithmica 5 (1990) 295-311.
</li>
<li>K. Culik, T. Ottman, and D. Wood, "Dense Multiway Trees," ACM
Transactions on Database Systems 6 (1981), 486-512.
</li>
<li>B. Eisenbath, N. Ziviana, G. H. Gonnet, K. Melhorn, and D. Wood,
"The Theory of Fringe Analysis and its Application to 2-3 Trees and B-trees," Information and Control 55 (1982), 125-174.
</li>
<li>J. L. Eppinger, "An Empirical Study of Insertion and Deletion in
Binary Search Trees," Communications of the ACM 26 (1983), 663-669.
</li>
<li>P. Flajolet and A. Odlyzko, "The Average Height of Binary Trees
and Other Simple Trees," Journal of Computer and System Sciences 25 (1982), 171-213.
</li>
<li>G. H. Gonnet and R. Baeza-Yates, Handbook of Algorithms and Data
Structures, second edition, Addison-Wesley, Reading, MA, 1991.
</li>
<li>E. Gudes and S. Tsur, "Experiments with B-tree Reorganization,"
Proceedings of ACM SIGMOD Symposium on Management of Data (1980), 200-206.
</li>
<li>L. J. Guibas and R. Sedgewick, "A Dichromatic Framework for
Balanced Trees," Proceedings of the Nineteenth Annual IEEE
Symposium on Foundations of Computer Science (1978), 8-21.
</li>
<li>T. H. Hibbard, "Some Combinatorial Properties of Certain Trees
with Applications to Searching and Sorting," Journal of the ACM 9 (1962), 13-28.
</li>
<li>A. T. Jonassen and D. E. Knuth, "A Trivial Algorithm Whose
Analysis Isn't," Journal of Computer and System Sciences 16 (1978), 301-322.
</li>
<li>P. L. Karlton, S. H. Fuller, R. E. Scroggs, and E. B. Kaehler,
"Performance of Height Balanced Trees," Communications of the ACM 19 (1976), 23-28.
</li>
<li>D. E. Knuth, The Art of Computer Programming: Volume 1:
Fundamental Algorithms, second edition, Addison-Wesley, Reading, MA, 1973.
</li>
<li>D. E. Knuth, The Art of Computer Programming: Volume 3: Sorting
and Searching, second printing, Addison-Wesley, Reading, MA, 1975.
</li>
<li>K. Melhorn, "A Partial Analysis of Height-Balanced Trees under
Random Insertions and Deletions," SIAM Journal of Computing 11 (1982), 748-760.
</li>
<li>K. Melhorn, Data Structures and Algorithms 1: Sorting and
Searching, Springer-Verlag, Berlin, 1984.
</li>
<li>J. Nievergelt and E. M. Reingold, "Binary Search Trees of Bounded
Balance," SIAM Journal on Computing 2 (1973), 33-43.
</li>
<li>A. J. Perlis and C. Thornton, "Symbol Manipulation in Threaded
Lists," Communications of the ACM 3 (1960), 195-204.
</li>
<li>D. D. Sleator and R. E. Tarjan, "Self-adjusting Binary Search
Trees," Journal of ACM 32 (1985), 652-686.
</li>
<li>D. D. Sleator, R. E. Tarjan, and W. P. Thurston, "Rotation
Distance, Triangulations, and Hyperbolic Geometry," Journal of AMS
(1988), 647-682.
</li>

<li>H. F. Smith, Data Structures-Form and Function, Harcourt Brace
Jovanovich, 1987.
</li>
<li>R. E. Tarjan, "Sequential Access in Splay Trees Takes Linear
Time," Combinatorica 5 (1985), 367-378.
</li>
<li>A. C. Yao, "On Random 2-3 trees," Acta Informatica 9 (1978), 159-170.
</li>
</ol>
</div>
</div>

<div id="outline-container-sec-2-4" class="outline-3">
<h3 id="sec-2-4">CHAPTER 5: HASHING</h3>
<div class="outline-text-3" id="text-2-4">
Despite the apparent simplicity of hashing, much of the analysis is quite difficult and there are
still many unresolved questions. There are also many interesting theoretical issues, which
generally attempt to make it unlikely that the worst-case
possibilities of hashing arise.<br>

An early paper on hashing is [17]. A wealth of information on the subject, including an analysis
of closed hashing with linear probing can be found in [11]. An excellent survey on the subject is
[14]; [15] contains suggestions, and pitfalls, for choosing hash functions. Precise analytic and
simulation results for all of the methods described in this chapter
can be found in [8].<br>

An analysis of double hashing can be found in [9] and [13]. Yet another collision resolution
scheme is coalesced hashing, as described in [18]. Yao [20] has shown that uniform hashing, in
which no clustering exists, is optimal with respect to cost of a successful search.<br>

If the input keys are known in advance, then perfect hash functions, which do not allow
collisions, exist [2], [7]. Some more complicated hashing schemes, for which the worst case
depends not on the particular input but on random numbers chosen by the algorithm, appear in [3]
and [4].<br>

Extendible hashing appears in [5], with analysis in [6] and [19].
One method of implementing Exercise 5.5 is described in [16]. Exercise 5.11 (a-d) is from [10].
Part (e) is from [12], and part (f) is from [1].
<ol class="org-ol">
<li>R. S. Boyer and J. S. Moore, "A Fast String Searching Algorithm,"
Communications of the ACM 20 (1977), 762-772.
</li>
<li>J. L. Carter and M. N. Wegman, "Universal Classes of Hash
Functions," Journal of Computer and System Sciences 18 (1979), 143-154.
</li>
<li>M. Dietzfelbinger, A. R. Karlin, K. Melhorn, F. Meyer auf der
Heide, H. Rohnert, and R. E. Tarjan, Dynamic Perfect Hashing: Upper
and Lower Bounds," Proceedings of the Twenty-ninth IEEE Symposium on Foundations of Computer Science (1988), 524-531.
</li>
<li>R. J. Enbody and H. C. Du, "Dynamic Hashing Schemes," Computing Surveys 20 (1988), 85-113.
</li>
<li>R. Fagin, J. Nievergelt, N. Pippenger, and H. R. Strong,
"Extendible Hashing-A Fast Access Method for Dynamic Files," ACM Transactions on Database Systems 4 (1979), 315-344.
</li>
<li>P. Flajolet, "On the Performance Evaluation of Extendible Hashing
and Trie Searching," Acta Informatica 20 (1983), 345-369.
</li>
<li>M. L. Fredman, J. Komlos, and E. Szemeredi, "Storing a Sparse Table
with O(1) Worst Case Access Time," Journal of the ACM 31 (1984), 538-544.
</li>
<li>G. H. Gonnet and R. Baeza-Yates, Handbook of Algorithms and Data
Structures, second edition, Addison-Wesley, Reading, MA, 1991.
</li>
<li>L. J. Guibas and E. Szemeredi, "The Analysis of Double Hashing,"
Journal of Computer and System Sciences 16 (1978), 226-274.
</li>
<li>R. M. Karp and M. O. Rabin, "Efficient Randomized Pattern-Matching
Algorithms," Aiken Computer Laboratory Report TR-31-81, Harvard University, Cambridge, MA, 1981.
</li>
<li>D. E. Knuth, The Art of Computer Programming, Vol 3: Sorting and
Searching, second printing, Addison-Wesley, Reading, MA, 1975.
</li>
<li>D. E. Knuth, J. H. Morris, V. R. Pratt, "Fast Pattern Matching in
Strings," SIAM Journal on Computing 6 (1977), 323-350.
</li>
<li>G. Lueker and M. Molodowitch, "More Analysis of Double Hashing,"
Proceedings of the Twentieth ACM Symposium on Theory of Computing (1988), 354-359.
</li>
<li>W. D. Maurer and T. G. Lewis, "Hash Table Methods," Computing Surveys 7 (1975), 5-20.
</li>
<li>B. J. McKenzie, R. Harries, and T. Bell, "Selecting a Hashing
Algorithm," Software&#x2013;Practice and Experience 20 (1990), 209-224.
</li>
<li>R. Morris, "Scatter Storage Techniques," Communications of the ACM 11 (1968), 38-44.
</li>
<li>W. W. Peterson, "Addressing for Random Access Storage," IBM
Journal of Research and Development 1 (1957), 130-146.
</li>
<li>J. S. Vitter, "Implementations for Coalesced Hashing,"
Communications of the ACM 25 (1982), 911-926.
</li>
<li>A. C. Yao, "A Note on The Analysis of Extendible Hashing,"
Information Processing Letters 11 (1980), 84-86.
</li>
<li>A. C. Yao, "Uniform Hashing is Optimal," Journal of the ACM 32
(1985), 687-693.
</li>
</ol>
</div>
</div>

<div id="outline-container-sec-2-5" class="outline-3">
<h3 id="sec-2-5">CHAPTER 6: PRIORITY QUEUES (HEAPS)</h3>
<div class="outline-text-3" id="text-2-5">
The binary heap was first described in [21]. The linear-time algorithm
for its construction is from [9].<br>

The first description of d -heaps was in [14]. Leftist heaps were invented by Crane [7] and
described in Knuth [15]. Skew heaps were developed by Sleator and Tarjan [17]. Binomial queues
were invented by Vuillemin [20]; Brown provided a detailed analysis and empirical study showing
that they perform well in practice [2], if carefully implemented.<br>

Exercise 6.7 (b-c) is taken from [12]. A method for constructing binary heaps that uses about
1.52n comparisons on average is described in [16]. Lazy deletion in leftist heaps (Exercise 6.21)
is from [6]. A solution to Exercise 6.33 can be found in [5].<br>

Min-max heaps (Exercise 6.15) were originally described in [1]. More efficient implementation of
the operations is given in [13] and [18]. An alternate representation for double ended priority
queues is the deap. Details can be found in [3] and [4].<br>

A theoretically interesting priority queue representation is the Fibonacci heap [11], which we
will describe in Chapter 11. The Fibonacci heap allows all operations to be performed in O(1)
amortized time, except for deletions, which are O(log n). Relaxed heaps [8] achieve identical
bounds in the worst case. Another interesting implementation is the pairing heap [10]. Finally, a priority queue that works when the data consists of small integers is described in [19].
<ol class="org-ol">
<li>M. D. Atkinson, J. R. Sack, N. Santoro, and T. Strothotte, "Min-Max
Heaps and Generalized Priority Queues," Communications of the ACM 29 (1986), 996-1000.
</li>
<li>M. R. Brown, "Implementation and Analysis of Binomial Queue
Algorithms," SIAM Journal on Computing 7 (1978), 298-319.
</li>
<li>S. Carlsson, "The Deap&#x2013;A Double-ended Heap to Implement
Double-ended Priority Queues," Information Processing Letters 26 (1987), 33-36.
</li>
<li>S. Carlsson, J. Chen, and T. Strothotte, "A Note on the
Construction of the Data Structure 'Deap'," Information Processing Letters 31 (1989), 315-317.
</li>
<li>S. Carlsson, J. I. Munro, and P. V. Poblete, "An Implicit Binomial
Queue with Constant Insertion Time," Proceedings of First Scandinavian Workshop on Algorithm Theory, 1988, 1-13.
</li>
<li>D. Cheriton and R. E. Tarjan, "Finding Minimum Spanning Trees,"
SIAM Journal on Computing 5 (1976), 724-742.
</li>
<li>C. A. Crane, "Linear Lists and Priority Queues as Balanced Binary
Trees," Technical Report STAN-CS-72-259, Computer Science Department, Stanford University, Stanford, CA, 1972.
</li>
<li>J. R. Driscoll, H. N. Gabow, R. Shrairman, and R. E. Tarjan,
"Relaxed Heaps: An Alternative to Fibonacci Heaps with Applications
to Parallel Computation," Communications of the ACM 31 (1988), 1343-1354.
</li>
<li>R. W. Floyd, "Algorithm 245: Treesort 3:", Communications of the ACM 7 (1964), 701.
</li>
<li>M. L. Fredman, R. Sedgewick, D. D. Sleator, and R. E. Tarjan, "The
Pairing Heap: A New Form of Self-adjusting Heap," Algorithmica 1 (1986), 111-129.
</li>
<li>M. L. Fredman and R. E. Tarjan, "Fibonacci Heaps and Their Uses in
Improved Network Optimization Algorithms," Journal of the ACM 34 (1987), 596-615.
</li>
<li>G. H. Gonnet and J. I. Munro, "Heaps on Heaps," SIAM Journal on Computing 15 (1986), 964-971.
</li>
<li>A. Hasham and J. R. Sack, "Bounds for Min-max Heaps," BIT 27 (1987), 315-323.
</li>
<li>D. B. Johnson, "Priority Queues with Update and Finding Minimum
Spanning Trees," Information Processing Letters 4 (1975), 53-57.
</li>
<li>D. E. Knuth, The Art of Computer Programming, Vol 3: Sorting and
Searching, second printing, Addison-Wesley, Reading, MA, 1975.
</li>
<li>C. J. H. McDiarmid and B. A. Reed, "Building Heaps Fast," Journal
of Algorithms 10 (1989), 352-365.
</li>
<li>D. D. Sleator and R. E. Tarjan, "Self-adjusting Heaps," SIAM
Journal on Computing 15 (1986), 52-69.
</li>
<li>T. Strothotte, P. Eriksson, and S. Vallner, "A Note on
Constructing Min-max Heaps," BIT 29 (1989), 251-256.
</li>
<li>P. van Emde Boas, R. Kaas, E. Zijlstra, "Design and Implementation
of an Efficient Priority Queue," Mathematical Systems Theory 10 (1977), 99-127.
</li>
<li>J. Vuillemin, "A Data Structure for Manipulating Priority Queues,"
Communications of the ACM 21 (1978), 309-314.
</li>
<li>J. W. J. Williams, "Algorithm 232: Heapsort," Communications of
the ACM 7 (1964), 347-348.
</li>
</ol>
</div>
</div>

<div id="outline-container-sec-2-6" class="outline-3">
<h3 id="sec-2-6">CHAPTER 7: SORTING</h3>
<div class="outline-text-3" id="text-2-6">
Knuth's book [10] is a comprehensive, though somewhat dated, reference for sorting. Gonnet and
Baeza-Yates [4] has some more recent results, as well as a huge
bibliography.<br>

The original paper detailing Shellsort is [21]. The paper by Hibbard [5] suggested the use of the
increments 2k - 1 and tightened the code by avoiding swaps. Theorem 7.4 is from [12]. Pratt's
lower bound, which uses a more complex method than that suggested in the text, can be found in
[14]. Improved increment sequences and upper bounds appear in [9], [20], and [23]; matching lower
bounds have been shown in [24]. A recent unpublished result by Poonen shows that no increment
sequence gives an O(n log n) worst-case running time. An identical result was obtained
independently and appears in [13]. The average-case running time for Shellsort is still
unresolved. Yao [26] has performed an extremely complex analysis for the three-increment case.
The result has yet to be extended to more increments. Experiments with various increment
sequences appear in [22].<br>

Heapsort was invented by Williams [25]; Floyd [1] provided the linear-time algorithm for heap
construction. The analysis of its average case has only recently been obtained [15].<br>

An exact average-case analysis of mergesort has been claimed in [3]; the paper detailing the
results is forthcoming. An algorithm to perform merging in linear time without extra space is
described in [8].<br>

Quicksort is from Hoare [6]. This paper analyzes the basic algorithm, describes most of the
improvements, and includes the selection algorithm. A detailed analysis and empirical study was
the subject of Sedgewick's dissertation [19]. Many of the important results appear in the three
papers [16], [17], and [18].<br>

Decision trees and sorting optimality are discussed in Ford and Johnson [2]. This paper also
provides an algorithm that almost meets the lower bound in terms of number of comparisons (but
not other operations). This algorithm was eventually shown to be slightly suboptimal by Manacher
[11].<br>

External sorting is covered in detail in [10]. Stable sorting, described in Exercise 7.24, has
been addressed by Horvath [7].<br>

<ol class="org-ol">
<li>R. W. Floyd, "Algorithm 245: Treesort 3," Communications of the ACM 7 (1964), 701.
</li>
<li>L. R. Ford and S. M. Johnson, "A Tournament Problem," American Mathematics Monthly 66 (1959),387-389.
</li>
<li>M. Golin and R. Sedgewick, "Exact Analysis of Mergesort," Fourth
SIAM Conference on Discrete Mathematics, 1988.
</li>
<li>G. H. Gonnet and R. Baeza-Yates, Handbook of Algorithms and Data
Structures, second edition, Addison-Wesley, Reading, MA, 1991.
</li>
<li>T. H. Hibbard, "An Empirical Study of Minimal Storage Sorting,"
Communications of the ACM 6
</li>
<li>C. A. R. Hoare, "Quicksort," Computer Journal 5 (1962), 10-15.
</li>
<li>E. C. Horvath, "Stable Sorting in Asymptotically Optimal Time and
Extra Space," Journal of the ACM 25 (1978), 177-199.
</li>
<li>B. Huang and M. Langston, "Practical In-place Merging,"
Communications of the ACM 31 (1988), 348-352.
</li>
<li>J. Incerpi and R. Sedgewick, "Improved Upper Bounds on Shellsort,"
Journal of Computer and System Sciences 31 (1985), 210-224.
</li>
<li>D. E. Knuth, The Art of Computer Programming. Volume 3: Sorting
and Searching, second printing, Addison-Wesley, Reading, MA, 1975.
</li>
<li>G. K. Manacher, "The Ford-Johnson Sorting Algorithm Is Not
Optimal," Journal of the ACM 26 (1979), 441-456.
</li>
<li>A. A. Papernov and G. V. Stasevich, "A Method of Information
Sorting in Computer Memories," Problems of Information Transmission 1 (1965), 63-75.
</li>
<li>C. G. Plaxton and T. Suel, "Improved Lower Bounds for Shellsort,"
Proceedings of the Thirtythird Annual IEEE Symposium on the Foundations of Computer Science, (1992).
</li>
<li>V. R. Pratt, Shellsort and Sorting Networks, Garland Publishing,
New York, 1979. (Originally presented as the author's Ph.D. thesis, Stanford University, 1971.)
</li>
<li>R. Schaffer and R. Sedgewick, "The Analysis of Heapsort," Journal of Algorithms, to appear.
</li>
<li>R. Sedgewick, "Quicksort with Equal Keys," SIAM Journal on Computing 6 (1977), 240-267.
</li>
<li>R. Sedgewick, "The Analysis of Quicksort Programs," Acta Informatica 7 (1977), 327-355.
</li>
<li>R. Sedgewick, "Implementing Quicksort Programs," Communications of
the ACM 21 (1978), 847- 857.
</li>
<li>R. Sedgewick, Quicksort, Garland Publishing, New York, 1978.
(Originally presented as the author's Ph.D. thesis, Stanford University, 1975.)
</li>
<li>R. Sedgewick, "A New Upper Bound for Shellsort," Journal of Algorithms 2 (1986), 159-173.
</li>
<li>D. L. Shell, "A High-Speed Sorting Procedure," Communications of the ACM 2 (1959), 30-32.
</li>
<li>M. A. Weiss, "Empirical Results on the Running Time of Shellsort,"
Computer Journal 34 (1991), 88-91.
</li>
<li>M. A. Weiss and R. Sedgewick, "More On Shellsort Increment
Sequences," Information Processing Letters 34 (1990), 267-270.
</li>
<li>M. A. Weiss and R. Sedgewick, "Tight Lower Bounds for Shellsort,"
Journal of Algorithms 11 (1990), 242-251.
</li>
<li>J. W. J. Williams, "Algorithm 232: Heapsort," Communications of the ACM 7 (1964), 347-348.
</li>
<li>A. C. Yao, "An Analysis of (h, k, 1) Shellsort," Journal of
Algorithms 1 (1980), 14-50.
</li>
</ol>
</div>
</div>

<div id="outline-container-sec-2-7" class="outline-3">
<h3 id="sec-2-7">CHAPTER 8: THE DISJOINT SET ADT</h3>
<div class="outline-text-3" id="text-2-7">
Various solutions to the union/find problem can be found in [5], [8], and [10]. Hopcroft and
Ullman showed the O(m log* n) bound of Section 8.6. Tarjan [14] obtained the bound O(m (m,n)).
A more precise (but asymptotically identical) bound for m < n appears in [2] and [17]. Various
other strategies for path compression and unions also achieve the same bound; see [17] for
details.<br>

A lower bound showing that under certain restrictions (m (m,n)) time is required to
process m union/find operations was given by Tarjan [15]. Identical bounds under less restrictive
conditions have been recently shown in [6] and [13].<br>

Applications of the union/find data structure appear in [1] and [9]. Certain special cases of the
union/find problem can be solved in O(m) time [7]. This reduces the running time of several
algorithms, such as [1], graph dominance, and reducibility (see references in Chapter 9) by a
factor of (m,n). Others, such as [9] and the graph connectivity problem in this chapter, are
unaffected. The paper lists 10 examples. Tarjan has used path compression to obtain efficient
algorithms for several graph problems [16].<br>

Average-case results for the union/find problem appear in [4], [11], and [19]. Results bounding
the running time of any single operation (as opposed to the entire sequence) appear in [3] and
[12].<br>

Exercise 8.8 is solved in [18].

<ol class="org-ol">
<li>A. V. Aho, J. E. Hopcroft, J. D. Ullman, "On Finding Lowest Common
Ancestors in Trees," SIAM Journal on Computing 5 (1976), 115-132.
</li>
<li>L. Banachowski, "A Complement to Tarjan's Result about the Lower
Bound on the Complexity of the Set Union Problem," Information Processing Letters 11 (1980), 59-65.
</li>
<li>N. Blum, "On the Single-operation Worst-case Time Complexity of the
Disjoint Set Union Problem," SIAM Journal on Computing 15 (1986), 1021-1024.
</li>
<li>J. Doyle and R. L. Rivest, "Linear Expected Time of a Simple Union
Find Algorithm," Information Processing Letters 5 (1976), 146-148.
</li>
<li>M. J. Fischer, "Efficiency of Equivalence Algorithms," Complexity
of Computer Computation (eds. R. E. Miller and J. W. Thatcher), Plenum Press, 1972, 153-168.
</li>
<li>M. L. Fredman and M. E. Saks, "The Cell Probe Complexity of Dynamic
Data Structures," Proceedings of the Twenty-first Annual Symposium on Theory of Computing (1989), 345-354.
</li>
<li>H. N. Gabow and R. E. Tarjan, "A Linear-time Algorithm for a
Special Case of Disjoint Set Union,"Journal of Computer and System Sciences 30 (1985), 209-221.
</li>
<li>B. A. Galler and M. J. Fischer, "An Improved Equivalence Algorithm," Communications of the ACM
</li>
</ol>
<p>
7 (1964), 301-303.
</p>
<ol class="org-ol">
<li>J. E. Hopcroft and R. M. Karp, "An Algorithm for Testing the
Equivalence of Finite Automata," Technical Report TR-71-114, Department of Computer Science, Cornell University, Ithaca, NY, 1971.
</li>
<li>J. E. Hopcroft and J. D. Ullman, "Set Merging Algorithms," SIAM
Journal on Computing 2 (1973), 294-303.
</li>
<li>D. E. Knuth and A. Schonhage, "The Expected Linearity of a Simple
Equivalence Algorithm," Theoretical Computer Science 6 (1978), 281-315.
</li>
<li>J. A. LaPoutre, "New Techniques for the Union-Find Problem,"
Proceedings of the First Annual ACM-SIAM Symposium on Discrete Algorithms (1990), 54-63.
</li>
<li>J. A. LaPoutre, "Lower Bounds for the Union-Find and the
Split-Find Problem on Pointer Machines," Proceedings of the Twenty
Second Annual ACM Symposium on Theory of Computing (1990), 34-44.
</li>
<li>R. E. Tarjan, "Efficiency of a Good but Not Linear Set Union
Algorithm," Journal of the ACM 22 (1975), 215-225.
</li>
<li>R. E. Tarjan, "A Class of Algorithms Which Require Nonlinear Time
to Maintain Disjoint Sets,"Journal of Computer and System Sciences 18 (1979), 110-127.
</li>
<li>R. E. Tarjan, "Applications of Path Compression on Balanced
Trees," Journal of the ACM 26 (1979), 690-715.
</li>
<li>R. E. Tarjan and J. van Leeuwen, "Worst Case Analysis of Set Union
Algorithms," Journal of the ACM 31 (1984), 245-281.
</li>
<li>J. Westbrook and R. E. Tarjan, "Amortized Analysis of Algorithms
for Set Union with Backtracking," SIAM Journal on Computing 18 (1989), 1-11.
</li>
<li>A. C. Yao, "On the Average Behavior of Set Merging Algorithms,"
Proceedings of Eighth Annual ACM Symposium on the Theory of Computation (1976), 192-195.
</li>
</ol>
</div>
</div>

<div id="outline-container-sec-2-8" class="outline-3">
<h3 id="sec-2-8">CHAPTER 9: GRAPH ALGORITHMS</h3>
<div class="outline-text-3" id="text-2-8">
Good graph theory textbooks include
[7], [12], [21], and [34]. More advanced topics, including
the more careful attention to running times, are covered in [36], [38], and [45].<br>

Use of adjacency lists was advocated in [23]. The topological sort algorithm is from [28], as
described in [31]. Dijkstra's algorithm appeared in [8]. The improvements using d-heaps and
Fibonacci heaps are described in [27] and [14], respectively. The shortest-path algorithm with
negative edge weights is due to Bellman [3]; Tarjan [45] describes a more efficient way to
guarantee termination.<br>

Ford and Fulkerson's seminal work on network flow is [13]. The idea of augmenting along shortest
paths or on paths admitting the largest flow increase is from [11]. Other approaches to the
problem can be found in [9], [30], and [20]. An algorithm for the min-cost flow problem can be
found in [18].<br>

An early minimum spanning tree algorithm can be found in [4]. Prim's algorithm is from [39];
Kruskal's algorithm appears in [32]. Two O(|E| log log |V|) algorithms are [5] and [46]. The
theoretically best-known algorithms appear in [14] and [16]. An empirical study of these
algorithms suggests that Prim's algorithm, implemented with decrease_key, is best in practice on
most graphs [37].<br>

The algorithm for biconnectivity is from [41]. The first linear-time strong components algorithm
(Exercise 9.28) appears in the same paper. The algorithm presented in the text is due to Kosaraju
(unpublished) and Sharir [40]. Other applications of depth-first search appear in [24], [25],
[42], and [43] (as mentioned in Chapter 8, the results in [42] and [43] have been improved, but
the basic algorithm is unchanged).<br>

The classic reference work for the theory of NP-complete problems is [19]. Additional material
can be found in [1]. The NP-completeness of satisfiability is shown in [6]. The other seminal
paper is [29], which showed the NP-completeness of 21 problems. An excellent survey of complexity
theory is [44]. An approximation algorithm for the traveling salesman problem, which generally
gives nearly optimal results, can be found in [35].<br>

A solution to Exercise 9.8 can be found in [2]. Solutions to the bipartite matching problem in
Exercise 9.13 can be found in [22] and [33]. The problem can be generalized by adding weights to
the edges and removing the restriction that the graph is bipartite. Efficient solutions for the
unweighted matching problem for general graphs are quite complex. Details can be found in [10],
[15], and [17].<br>

Exercise 9.35 deals with planar graphs, which commonly arise in practice. Planar graphs are very
sparse, and many difficult problems are easier on planar graphs. An example is the graph
isomorphism problem, which is solvable in linear time for planar graphs [26]. No polynomial time
algorithm is known for general graphs.<br>

<ol class="org-ol">
<li>A. V. Aho, J. E. Hopcroft, and J. D. Ullman, The Design and
Analysis of Computer Algorithms, Addison-Wesley, Reading, MA, 1974.
</li>
<li>R. K. Ahuja, K. Melhorn, J. B. Orlin, and R. E. Tarjan, "Faster
Algorithms for the Shortest Path Problem," Journal of the ACM 37 (1990), 213-223.
</li>
<li>R. E. Bellman, "On a Routing Problem," Quarterly of Applied Mathematics 16 (1958), 87-90.
</li>
<li>O. Boruvka, "Ojistém problému minimálním (On a Minimal Problem),"
Práca Moravské 3 (1926), 37-58.
</li>
<li>D. Cheriton and R. E. Tarjan, "Finding Minimum Spanning Trees,"
SIAM Journal on Computing 5 (1976), 724-742.
</li>
<li>S. Cook, "The Complexity of Theorem Proving Procedures,"
Proceedings of the Third Annual ACM Symposium on Theory of Computing (1971), 151-158.
</li>
<li>N. Deo, Graph Theory wtth Applications to Engineering and Computer
Science, Prentice Hall, Englewood Cliffs, NJ, 1974.
</li>
<li>E. W. Dijkstra, "A Note on Two Problems in Connexion with Graphs,"
Numerische Mathematik 1 (1959), 269-271.
</li>
<li>E. A. Dinic, "Algorithm for Solution of a Problem of Maximum Flow
in Networks with Power Estimation," Soviet Mathematics Doklady 11 (1970), 1277-1280.
</li>
<li>J. Edmonds, "Paths, Trees, and Flowers," Canadian Journal of Mathematics 17 (1965) 449-467.
</li>
<li>J. Edmonds and R. M. Karp, "Theoretical Improvements in
Algorithmic Efficiency for Network Flow Problems," Journal of the ACM 19 (1972), 248-264.
</li>
<li>S. Even, Graph Algorithms, Computer Science Press, Potomac, MD, 1979.
</li>
<li>L. R. Ford, Jr. and D. R. Fulkerson, Flows in Networks, Princeton
University Press, Princeton, NJ, 1962.
</li>
<li>M. L. Fredman and R. E. Tarjan, "Fibonacci Heaps and Their Uses in
Improved Network Optimization Algorithms," Journal of the ACM 34 (1987), 596-615.
</li>
<li>H. N. Gabow, "Data Structures for Weighted Matching and Nearest
Common Ancestors with Linking," Proceedings of First Annual
ACM-SIAM Symposium on Discrete Algorithms (1990), 434-443.
</li>
<li>H. N. Gabow, Z. Galil, T. H. Spencer, and R. E. Tarjan, "Efficient
Algorithms for Finding Minimum Spanning Trees on Directed and Undirected Graphs," Combinatorica 6 (1986), 109-122.
</li>
<li>Z. Galil, "Efficient Algorithms for Finding Maximum Matchings in
Graphs," ACM Computing Surveys 18 (1986), 23-38.
</li>
<li>Z. Galil and E. Tardos,"An O(n2(m + n log n)log n) Min-Cost Flow
Algorithm," Journal of the ACM 35 (1988), 374-386.
</li>
<li>M. R. Garey and D. S. Johnson, Computers and Intractability: A
Guide to the Theory of NPCompleteness, Freeman, San Francisco, 1979.
</li>
<li>A. V. Goldberg and R. E. Tarjan, "A New Approach to the
Maximum-Flow Problem," Journal of the ACM 35 (1988), 921-940.
</li>
<li>F. Harary, Graph Theory, Addison-Wesley, Reading, MA, 1969.
</li>
<li>J. E. Hopcroft and R. M. Karp, "An n5/2 Algorithm for Maximum
Matchings in Bipartite Graphs," SIAM Journal on Computing 2 (1973), 225-231.
</li>
<li>J. E. Hopcroft and R. E. Tarjan, "Algorithm 447: Efficient
Algorithms for Graph Manipulation," Communications of the ACM 16 (1973), 372-378.
</li>
<li>J. E. Hopcroft and R. E. Tarjan, "Dividing a Graph into
Triconnected Components," SIAM Journal on Computing 2 (1973), 135-158.
</li>
<li>J. E. Hopcroft and R. E. Tarjan, "Efficient Planarity Testing," Journal of the ACM 21 (1974),
</li>
<li>J. E. Hopcroft and J. K. Wong, "Linear Time Algorithm for
Isomorphism of Planar Graphs," Proceedings of the Sixth Annual ACM Symposium on Theory of Computing (1974), 172-184.
</li>
<li>D. B. Johnson, "Efficient Algorithms for Shortest Paths in Sparse
Networks," Journal of the ACM 24 (1977), 1-13.
</li>
<li>A. B. Kahn, "Topological Sorting of Large Networks,"
Communications of the ACM 5 (1962), 558- 562.
</li>
<li>R. M. Karp, "Reducibility among Combinatorial Problems,"
Complexity of Computer Computations (eds. R. E. Miller and J. W. Thatcher), Plenum Press, New York, 1972, 85-103.
</li>
<li>A. V. Karzanov, "Determining the Maximal Flow in a Network by the
Method of Preflows," Soviet Mathematics Doklady 15 (1974), 434-437.
</li>
<li>D. E. Knuth, The Art of Computer Programming, Vol. 1: Fundamental
Algorithms, second edition, Addison-Wesley, Reading, MA, 1973.
</li>
<li>J. B. Kruskal, Jr. "On the Shortest Spanning Subtree of a Graph
and the Traveling Salesman Problem," Proceedings of the American Mathematical Society 7 (1956), 48-50.
</li>
<li>H. W. Kuhn, "The Hungarian Method for the Assignment Problem,"
Naval Research Logistics Quarterly 2 (1955), 83-97.
</li>
<li>E. L. Lawler, Combinatorial Optimization: Networks and Matroids,
Holt, Reinhart, and Winston, New York, NY, 1976.
</li>
<li>S. Lin and B. W. Kernighan, "An Effective Heuristic Algorithm for
the Traveling Salesman Problem," Operations Research 21 (1973), 498-516.
</li>
<li>K. Melhorn, Data Structures and Algorithms 2: Graph Algorithms and
NP-completeness, Springer- Verlag, Berlin, 1984.
</li>
<li>B. M. E. Moret and H. D. Shapiro, "An Empirical Analysis of
Algorithms for Constructing a Minimum Spanning Tree," Proceedings
of the Second Workshop on Algorithms and Data Structures (1991), 400-411.
</li>
<li>C. H. Papadimitriou and K. Steiglitz, Combinatorial Optimization:
Algorithms and Complexity, Prentice Hall, Englewood Cliffs, NJ, 1982.
</li>
<li>R. C. Prim, "Shortest Connection Networks and Some
Generalizations," Bell System Technical Journal 36 (1957), 1389-1401.
</li>
<li>M. Sharir, "A Strong-Connectivity Algorithm and Its Application in
Data Flow Analysis," Computers and Mathematics with Applications 7 (1981), 67-72.
</li>
<li>R. E. Tarjan, "Depth First Search and Linear Graph Algorithms,"
SIAM Journal on Computing 1 (1972), 146-160.
</li>
<li>R. E. Tarjan, "Testing Flow Graph Reducibility," Journal of
Computer and System Sciences 9 (1974), 355-365.
</li>
<li>R. E. Tarjan, "Finding Dominators in Directed Graphs," SIAM
Journal on Computing 3 (1974), 62-89.
</li>
<li>R. E. Tarjan, "Complexity of Combinatorial Algorithms," SIAM Review 20 (1978), 457-491.
</li>
<li>R. E. Tarjan, Data Structures and Network Algorithms, Society for
Industrial and Applied Mathematics, Philadelphia, PA, 1983.
</li>
<li>A. C. Yao, "An O( |E | log log |V | ) Algorithm for Finding
Minimum Spanning Trees," Information Processing Letters 4 (1975), 21-23.
</li>
</ol>
</div>
</div>

<div id="outline-container-sec-2-9" class="outline-3">
<h3 id="sec-2-9">CHAPTER 10: ALGORITHM DESIGN TECHNIQUES</h3>
<div class="outline-text-3" id="text-2-9">
<p>
#begin<sub>html</sub>
The original paper on Huffman codes is <sup><a id="fnr.1" name="fnr.1" class="footref" href="#fn.1">1</a></sup>. Variations on the
algorithm are discussed in <sup><a id="fnr.2" name="fnr.2" class="footref" href="#fn.2">2</a></sup>, <sup><a id="fnr.3" name="fnr.3" class="footref" href="#fn.3">3</a></sup>, and <sup><a id="fnr.4" name="fnr.4" class="footref" href="#fn.4">4</a></sup>. Another popular compression scheme is Ziv-Lempel encoding <sup><a id="fnr.5" name="fnr.5" class="footref" href="#fn.5">5</a></sup>, <sup><a id="fnr.6" name="fnr.6" class="footref" href="#fn.6">6</a></sup>. Here the
codes have a fixed length but represent strings instead of characters. <sup><a id="fnr.7" name="fnr.7" class="footref" href="#fn.7">7</a></sup> and <sup><a id="fnr.8" name="fnr.8" class="footref" href="#fn.8">8</a></sup> are good
surveys of the common compression schemes.&lt;br&gt;
</p>

<p>
The analysis of bin-packing heuristics first appeared in Johnson's Ph.D. thesis and was published
in <sup><a id="fnr.9" name="fnr.9" class="footref" href="#fn.9">9</a></sup>. The improved lower bound for on-line bin packing given in Exercise 10.8 is from <sup><a id="fnr.10" name="fnr.10" class="footref" href="#fn.10">10</a></sup>;
this result has been improved further in <sup><a id="fnr.11" name="fnr.11" class="footref" href="#fn.11">11</a></sup>. <sup><a id="fnr.12" name="fnr.12" class="footref" href="#fn.12">12</a></sup> describes another approach to on-line bin
packing.&lt;br&gt;
</p>

<p>
Theorem 10.7 is from <sup><a id="fnr.13" name="fnr.13" class="footref" href="#fn.13">13</a></sup>. The closest points algorithm appeared in <sup><a id="fnr.14" name="fnr.14" class="footref" href="#fn.14">14</a></sup>. <sup><a id="fnr.15" name="fnr.15" class="footref" href="#fn.15">15</a></sup> describes the
turnpike reconstruction problem and its applications. Two books on the relatively new field of
computational geometry are <sup><a id="fnr.16" name="fnr.16" class="footref" href="#fn.16">16</a></sup> and <sup><a id="fnr.17" name="fnr.17" class="footref" href="#fn.17">17</a></sup>. <sup><a id="fnr.18" name="fnr.18" class="footref" href="#fn.18">18</a></sup> contains the lecture notes for a computational
geometry course taught at MIT; it includes an extensive bibliography.&lt;br&gt;
</p>

<p>
The linear-time selection algorithm appeared in <sup><a id="fnr.19" name="fnr.19" class="footref" href="#fn.19">19</a></sup>. <sup><a id="fnr.20" name="fnr.20" class="footref" href="#fn.20">20</a></sup> discusses the sampling approach that
finds the median in 1.5n expected comparisons. The O(n1.59) multiplication is from <sup><a id="fnr.21" name="fnr.21" class="footref" href="#fn.21">21</a></sup>.
Generalizations are discussed in <sup><a id="fnr.22" name="fnr.22" class="footref" href="#fn.22">22</a></sup> and <sup><a id="fnr.23" name="fnr.23" class="footref" href="#fn.23">23</a></sup>. Strassen's algorithm appears in the short paper
</p>
<p>
The discussion of random number generators is based on <sup><a id="fnr.24" name="fnr.24" class="footref" href="#fn.24">24</a></sup>. Park and Miller attribute the
portable implementation to Schrage <sup><a id="fnr.25" name="fnr.25" class="footref" href="#fn.25">25</a></sup>. Skip lists are discussed by Pugh in <sup><a id="fnr.26" name="fnr.26" class="footref" href="#fn.26">26</a></sup>. The randomized
primality-testing algorithm is due to Miller <sup><a id="fnr.27" name="fnr.27" class="footref" href="#fn.27">27</a></sup> and Rabin <sup><a id="fnr.28" name="fnr.28" class="footref" href="#fn.28">28</a></sup>. The theorem that at most (n -
9)/4 values of a fool the algorithm is from Monier <sup><a id="fnr.29" name="fnr.29" class="footref" href="#fn.29">29</a></sup>. Other randomized algorithms are
discussed in <sup><a id="fnr.30" name="fnr.30" class="footref" href="#fn.30">30</a></sup>.&lt;br&gt;
</p>

<p>
More information on - pruning can be found in <sup><a id="fnr.31" name="fnr.31" class="footref" href="#fn.31">31</a></sup>, <sup><a id="fnr.32" name="fnr.32" class="footref" href="#fn.32">32</a></sup>, and <sup><a id="fnr.33" name="fnr.33" class="footref" href="#fn.33">33</a></sup>. The top programs that
play chess, checkers, Othello, and backgammon have all achieved world class status. <sup><a id="fnr.34" name="fnr.34" class="footref" href="#fn.34">34</a></sup>
describes an Othello program. The paper appears in a special issue on computer games (mostly
chess); this issue is a gold mine of ideas. One of the papers describes the use of dynamic
programming to solve chess endgames completely when only a few pieces are left on the board.
Related research has resulted in the change of the 50-move rule in certain cases.
Exercise 10.41 is solved in <sup><a id="fnr.35" name="fnr.35" class="footref" href="#fn.35">35</a></sup>. It is the only known case of a homometric point set with no
duplicate distances. Determining whether any others exist for n &gt; 6 is open. Christofides <sup><a id="fnr.36" name="fnr.36" class="footref" href="#fn.36">36</a></sup>
gives a solution to Exercise 10.47, and also an algorithm which generates a tour at most
optimal. Exercise 10.52 is discussed in <sup><a id="fnr.37" name="fnr.37" class="footref" href="#fn.37">37</a></sup>. Exercise 10.55 is solved in <sup><a id="fnr.38" name="fnr.38" class="footref" href="#fn.38">38</a></sup>. An O(kn)
algorithm is given in <sup><a id="fnr.39" name="fnr.39" class="footref" href="#fn.39">39</a></sup>. Exercise 10.57 is discussed in <sup><a id="fnr.40" name="fnr.40" class="footref" href="#fn.40">40</a></sup>, but do not be misled by the
title of the paper.
#+end<sub>html</sub>
</p>


<ol class="org-ol">
<li>B. Abramson, "Control Strategies for Two-Player Games," ACM
Computing Surveys, 21 (1989), 137- 161.
</li>
<li>A. Aggarwal and J. Wein, Computational Geometry: Lecture Notes for
18.409, MIT Laboratory for Computer Science, 1988.
</li>
<li>T. Bell, I. H. Witten, and J. G. Cleary, "Modeling for Text
Compression," ACM Computing Surveys, 21 (1989), 557-591.
</li>
<li>R. E. Bellman, Dynamic Programming, Princeton University Press, Princeton, NJ, 1957.
</li>
<li>R. E. Bellman and S. E. Dreyfus, Applied Dynamic Programming,
Princeton University Press, Princeton, NJ, 1962.
</li>
<li>J. L. Bentley, D. Haken, and J. B. Saxe, "A General Method for
Solving Divide-and-Conquer Recurrences," SIGACT News, 12 (1980), 36-44.
</li>
<li>G. S. Bloom, "A Counterexample to the Theorem of Piccard," Journal
of Combinatorial Theory A (1977), 378-379.
</li>
<li>M. Blum, R. W. Floyd, V. R. Pratt, R. L. Rivest, and R. E. Tarjan,
"Time Bounds for Selection," Journal of Computer and System Sciences 7 (1973), 448-461.
</li>
<li>A. Borodin and J. I. Munro, The Computational Complexity of
Algebraic and Numerical Problems, American Elsevier, New York, 1975.
</li>
<li>L. Chang and J. Korsh, "Canonical Coin Changing and Greedy
Solutions," Journal of the ACM 23 (1976), 418-422.
</li>
<li>N. Christofides, "Worst-case Analysis of a New Heuristic for the
Traveling Salesman Problem," Management Science Research Report #388, Carnegie-Mellon University, Pittsburgh, PA, 1976.
</li>
<li>D. Coppersmith and S. Winograd, "Matrix Multiplication via
Arithmetic Progressions," Proceedings of the Nineteenth Annual ACM Symposium of the Theory of Computing (1987), 1-6.
</li>
<li>H. Edelsbrunner, Algorithms in Combinatorial Geometry, Springer-Verlag, Berlin, 1987.
</li>
<li>D. Eppstein, Z. Galil, R. Giancarlo, "Speeding up Dynamic
Programming," Proceedings of the Twenty-ninth Annual IEEE Symposium on the Foundations of Computer Science, (1988), 488-495.
</li>
<li>R. W. Floyd, "Algorithm 97: Shortest Path," Communications of the ACM 5 (1962), 345.
</li>
<li>R. W. Floyd and R. L. Rivest, "Expected Time Bounds for
Selection," Communications of the ACM 18 (1975), 165-172.
</li>
<li>M. L. Fredman, "New Bounds on the Complexity of the Shortest Path
Problem," SIAM Journal on Computing 5 (1976), 83-89.
</li>
<li>S. Godbole, "On Efficient Computation of Matrix Chain Products,"
IEEE Transactions on Computers 9 (1973), 864-866.
</li>
<li>T. C. Hu and M. R. Shing, "Computations of Matrix Chain Products,
Part I," SIAM Journal on Computing 11 (1982), 362-373.
</li>
<li>D. A. Huffman, "A Method for the Construction of Minimum
Redundancy Codes," Proceedings of the IRE 40 (1952), 1098-1101.
</li>
<li>D. S. Johnson, A. Demers, J. D. Ullman, M. R. Garey, and R. L.
Graham, "Worst-case Performance Bounds for Simple One-Dimensional
Packing Algorithms," SIAM Journal on Computing, 3 (1974), 299-325.
</li>
<li>A. Karatsuba and Y. Ofman, "Multiplication of Multi-digit Numbers
on Automata," Doklady Akademii Nauk SSSR 145 (1962), 293-294.
</li>
<li>D. E. Knuth, The Art of Computer Programming, Vol 2: Seminumerical
Algorithms, second edition, Addison-Wesley, Reading, MA, 1981.
</li>
<li>D. E. Knuth, "Optimum Binary Search Trees," Acta Informatica 1 (1971), 14-25.
</li>
<li>D. E. Knuth and R. W. Moore, "Estimating the Efficiency of
Backtrack Programs," Mathematics of Computation 29, (1975) 121-136.
</li>
<li>D. E. Knuth, "An Analysis of Alpha-Beta Cutoffs," Artificial Intelligence 6 (1975), 293-326.
</li>
<li>D. E. Knuth, TEX and Metafont, New Directions in Typesetting,
Digital Press, Bedford, MA, 1981.
</li>
<li>D. E. Knuth, "Dynamic Huffman Coding,"Journal of Algorithms 6 (1985), 163-180.
</li>
<li>G. M. Landau and U. Vishkin, "Introducing Efficient Parallelism
into Approximate String Matching and a New Serial Algorithm,"
Proceedings of the Eighteenth Annual ACM Symposium on Theory of Computing (1986), 220-230.
</li>
<li>L. L. Larmore, "Height-Restricted Optimal Binary Trees," SlAM
Journal on Computing 16 (1987), 1115-1123.
</li>
<li>L. L. Larmore and D. S. Hirschberg, "A Fast Algorithm for Optimal
Length-Limited Huffman Codes," Journal of the ACM 37 (1990), 464-473.
</li>
<li>K. Lee and S. Mahajan, "The Development of a World Class Othello
Program," Artificial Intelligence 43 (1990), 21-36.
</li>
<li>D. A. Lelewer and D. S. Hirschberg, "Data Compression," ACM
Computing Surveys 19 (1987), 261- 296.
</li>
<li>F. M. Liang, "A Lower Bound for On-line Bin Packing," Information
Processing Letters 10 (1980), 76-79.
</li>
<li>G. L. Miller, "Riemann's Hypothesis and Tests for Primality,"
Journal of Computer and System Sciences 13 (1976), 300-317.
</li>
<li>L. Monier, "Evaluation and Comparison of Two Efficient
Probabilistic Primality Testing Algorithms," Theoretical Computer Science 12 (1980), 97-108.
</li>
<li>V. Pan, "Strassen's Algorithm is Not Optimal," Proceedings of the
Nineteenth Annual IEEE Symposium on the Foundations of Computer Science (1978), 166-176.
</li>
<li>S. K. Park and K. W. Miller, "Random Number Generators: Good Ones
are Hard To Find," Communications of the ACM 31 (1988), 1192-1201.
</li>
<li>F. P. Preparata and M. I. Shamos, Computational Geometry: An
Introduction, Springer-Verlag, New York, NY, 1985.
</li>
<li>W. Pugh, "Skip Lists: A Probabilistic Alternative to Balanced
Trees," Communications of the ACM 33 (1990), 668-676.
</li>
<li>M. O. Rabin, "Probabilistic Algorithms," in Algorithms and
Complexity, Recent Results and New Directions (J. F. Traub, ed.), Academic Press, New York, 1976, 21-39.
</li>
<li>M. O. Rabin, "Probabilistic Algorithms for Testing Primality,"
Journal of Number Theory, 12 (1980), 128-138.
</li>
<li>P. Ramanan, D. J. Brown, C. C. Lee, and D. T. Lee, "On-line Bin
Packing in Linear Time," Journal of Algorithms 10 (1989), 305-326.
</li>
<li>M. I. Shamos and D. Hoey, "Closest-Point Problems," Proceedings of
the Sixteenth Annual IEEE Symposium on the Foundations of Computer Science (1975), 151-162.
</li>
<li>L. Schrage, "A More Portable FORTRAN Random Number Generator," ACM
Transactions on Mathematics Software 5 (1979), 132-138.
</li>
<li>S. S. Skiena, W. D. Smith, and P. Lemke, "Reconstructing Sets From
Interpoint Distances," Proceedings of the Sixth Annual ACM Symposium on Computational Geometry (1990), 332-339.
</li>
<li>V. Strassen, "Gaussian Elimination is Not Optimal," Numerische Mathematik 13 (1969), 354-356.
</li>
<li>R. A. Wagner and M. J. Fischer, "The String-to-String Correction
Problem," Journal of the ACM 21 (1974), 168-173.
</li>
<li>A. C. Yao, "New Algorithms for Bin Packing," Journal of the ACM 27 (1980), 207-227.
</li>
<li>F. F. Yao, "Efficient Dynamic Programming Using Quadrangle
Inequalities," Proceedings of the Twelfth Annual ACM Symposium on
the Theory of Computing (1980), 429-435.
</li>
<li>J. Ziv and A. Lempel, "A Universal Algorithm for Sequential Data
Compression," IEEE Transactions on Information Theory IT23 (1977), 337-343.
</li>
<li>J. Ziv and A. Lempel, "Compression of Individual Sequences via
Variable-rate Coding," IEEE Transactions on Information Theory IT24 (1978), 530-536.
</li>
</ol>
</div>
</div>

<div id="outline-container-sec-2-10" class="outline-3">
<h3 id="sec-2-10">CHAPTER 11: AMORTIZED ANALYSIS</h3>
<div class="outline-text-3" id="text-2-10">
An excellent survey of amortized analysis is provided in [9].<br>

Most of the references below duplicate citations in earlier chapters. We cite them again for
convenience and completeness. Binomial queues were first described in [10] and analyzed in [1].
Solutions to 11.3 and 11.4 appear in [8]. Fibonacci heaps are described in [3]. Exercise 11.9a
shows that splay trees are optimal, to within a constant factor of the the best static search
trees. 11.9b shows that splay trees are optimal, to within a constant factor of the best optimal
search trees. These, as well as two other strong results, are proved in the original splay tree
paper [6].<br>

The merge operation for splay trees is described in [5]. Exercise 11.12 is solved, with an
implicit use of amortization, in [2]. The paper also shows how to merge 2-3 trees efficiently. A
solution to 11.13 can be found in [4].<br>

Amortized analysis is used in [7] to design an on-line algorithm that processes a series of
queries in time only a constant factor larger than any off-line algorithm in its class.

<ol class="org-ol">
<li>M. R. Brown, "Implementation and Analysis of Binomial Queue
Algorithms," SIAM Journal on Computing 7 (1978), 298-319.
</li>
<li>M. R. Brown and R. E. Tarjan, "Design and Analysis of a Data
Structure for Representing Sorted Lists," SIAM Journal on Computing 9 (1980), 594-614.
</li>
<li>M. L. Fredman and R. E. Tarjan, "Fibonacci Heaps and Their Uses in
Improved Network Optimization Algorithms," Journal of the ACM 34 (1987), 596-615.
</li>
<li>H. Gajewska and R. E. Tarjan, "Deques with Heap Order," Information
Processing Letters 22 (1986), 197-200.
</li>
<li>G. Port and A. Moffat, "A Fast Algorithm for Melding Splay Trees,"
Proceedings of the First Workshop on Algorithms and Data Structures, 1989, 450-459.
</li>
<li>D. D. Sleator and R. E. Tarjan, "Self-adjusting Binary Search
Trees," Journal of the ACM 32 (1985), 652-686.
</li>
<li>D. D. Sleator and R. E. Tarjan, "Amortized Efficiency of List
Update and Paging Rules," Communications of the ACM 28 (1985), 202-208.
</li>
<li>D. D. Sleator and R. E. Tarjan, "Self-adjusting Heaps," SIAM
Journal on Computing 15 (1986), 52-69.
</li>
<li>R. E. Tarjan, "Amortized Computational Complexity," SIAM Journal on
Algebraic and Discrete Methods 6 (1985), 306-318.
</li>
<li>J. Vuillemin, "A Data Structure for Manipulating Priority Queues,"
Communications of the ACM 21 (1978), 309-314.
</li>
</ol>
</div>
</div>
</div>

<div id="outline-container-sec-3" class="outline-2">
<h2 id="sec-3">Notes</h2>
<div class="outline-text-2" id="text-3">
</div><div id="outline-container-sec-3-1" class="outline-3">
<h3 id="sec-3-1">Chapter 1: INTRODUCTION</h3>
<div class="outline-text-3" id="text-3-1">
</div><ul class="org-ul"><li><a id="sec-3-1-1" name="sec-3-1-1"></a>1.2.3<br  /><div class="outline-text-4" id="text-3-1-1">
<p>
<a href="http://orgmode.org/org.html#LaTeX-fragments">http://orgmode.org/org.html#LaTeX-fragments</a>
</p>

<p>
&sum;<sub>i=1</sub><sup>N</sup> i<sup>k</sup> &asymp; \frac{N<sup>k+1</sup>}{\abs{k+1}}  k &ne; -1
</p>
</div>
</li>

<li><a id="sec-3-1-2" name="sec-3-1-2"></a>1.3<br  /><div class="outline-text-4" id="text-3-1-2">
<p>
When writhing recursive routines, it is crucial to keep in mind the
four basic rules of recursion:
</p>
<ol class="org-ol">
<li><b>Base cases</b>. You must always have some base cases, which can be
solved without recursion.
</li>
<li><b>Making progress</b>. For the cases that are to be solved recursively,
the recursive call must always be to a case that makes progress
toward a base class.
</li>
<li><b>Design rule</b>. Assume that all the recursive calls work.
</li>
<li><b>Compound interest rule</b>. Never duplicate work by solving the same
instance of a problem in separate recursive calls.
</li>
</ol>
</div>
</li>
<li><a id="sec-3-1-3" name="sec-3-1-3"></a>1.5.2<br  /><div class="outline-text-4" id="text-3-1-3">
<p>
To summarize the parameter-passing options:
</p>
<ul class="org-ul">
<li>Call by value is appropriate for small objects that should not be
altered by the function.
</li>
<li>Call by constant reference is appropriate for large objects that
should not be altered by the function.
</li>
<li>Call by reference is appropriate for all objects that may be altered
by the function.
</li>
</ul>
</div>
</li>

<li><a id="sec-3-1-4" name="sec-3-1-4"></a>1.5.5<br  /><div class="outline-text-4" id="text-3-1-4">
<p>
This is a so-called <i>shallow copy</i>. Typically, we would expect a <i>deep
copy</i>, in which a clone of the entire object is made.
</p>
</div>
</li></ul>
</div>


<div id="outline-container-sec-3-2" class="outline-3">
<h3 id="sec-3-2">CHAPTER 2: ALGORITHM ANALYSIS</h3>
<div class="outline-text-3" id="text-3-2">
</div><ul class="org-ul"><li><a id="sec-3-2-1" name="sec-3-2-1"></a>2.1<br  /><div class="outline-text-4" id="text-3-2-1">
<p>
log<sup>k</sup> N = O(N) for any constant k. 
</p>

<p>
compute lim<sub>N&rarr;&infin;</sub> f(N)/g(N), using L<sup>'Hopital's</sup> rule if
necessary.
</p>

<p>
The rule is if lim<sub>N&rarr;&infin;</sub> f(N) = &infin; and
lim<sub>N&rarr;&infin;</sub> g(N) = &infin;, then lim<sub>N&rarr;&infin;</sub>
f(N)/g(N) = lim<sub>N&rarr;&infin;</sub>f<sup>'</sup>(N)/g<sup>'</sup>(N).
</p>

<p>
The limit can have four possible values:
</p>
<ul class="org-ul">
<li>The limit is 0: This means that f(N) = o(g(N)).
</li>
<li>The limit is c &ne; 0: This means that f(N) = &Theta;(g(N)).
</li>
<li>The limit is &infin;: This means that g(N) = o(f(N)).
</li>
<li>The limit is oscillates: There is no relation. 
</li>
</ul>
</div>
</li>

<li><a id="sec-3-2-2" name="sec-3-2-2"></a>2.4.2<br  /><div class="outline-text-4" id="text-3-2-2">
<ul class="org-ul">
<li>RULE 1-FOR LOOPS:
</li>
</ul>
<p>
The running time of a for loop is at most the running time of the statements
inside the for loop (including tests) times the number of iterations.
</p>

<ul class="org-ul">
<li>RULE 2-NESTED FOR LOOPS:
</li>
</ul>
<p>
Analyze these inside out. The total running time of a statement inside a group of
nested for loops is the running time of the statement multiplied by the product
of the sizes of all the for loops.
</p>

<ul class="org-ul">
<li>RULE 3-CONSECUTIVE STATEMENTS:
</li>
</ul>
<p>
These just add (which means that the maximum is the one that counts &#x2013; see 1(a)
on page 16).
</p>

<ul class="org-ul">
<li>RULE 4-lF/ELSE:
</li>
</ul>
<p>
For the fragment
</p>
<div class="org-src-container">

<pre class="src src-c"><span style="color: #00ffff;">if</span>( cond )
S1
<span style="color: #00ffff;">else</span>
S2
</pre>
</div>
<p>
the running time of an if/else statement is never more than the running time of
the test plus the larger of the running times of S1 and S2.
</p>
</div>
</li>
<li><a id="sec-3-2-3" name="sec-3-2-3"></a>2.4.3<br  /><div class="outline-text-4" id="text-3-2-3">
<ul class="org-ul">
<li>Algorithm 1 (O(N<sup>3</sup>))
</li>
</ul>
<div class="org-src-container">

<pre class="src src-c++"><span style="color: #98fb98;">int</span> <span style="color: #87cefa;">maxSubSum1</span>(<span style="color: #00ffff;">const</span> <span style="color: #98fb98;">vector</span>&lt;<span style="color: #98fb98;">int</span>&gt; &amp;<span style="color: #eedd82;">a</span>)
{
  <span style="color: #98fb98;">int</span> <span style="color: #eedd82;">maxSum</span> = 0;
  <span style="color: #00ffff;">for</span>(<span style="color: #98fb98;">int</span> <span style="color: #eedd82;">i</span> = 0; i &lt; a.size(); i++)
    <span style="color: #00ffff;">for</span>(<span style="color: #98fb98;">int</span> <span style="color: #eedd82;">j</span> = i; j &lt; a.size(); j++){
      <span style="color: #98fb98;">int</span> <span style="color: #eedd82;">thisSum</span> = 0;
      <span style="color: #00ffff;">for</span>(<span style="color: #98fb98;">int</span> <span style="color: #eedd82;">k</span> = i; k &lt;= j; k++){
        thisSum += a[k];
      }
      <span style="color: #00ffff;">if</span>(maxSum &lt; thisSum)
        maxSum = thisSum;
    }
  <span style="color: #00ffff;">return</span> maxSum;
}
</pre>
</div>

<ul class="org-ul">
<li>Algorthm 2 (O(N<sup>2</sup>))
</li>
</ul>
<div class="org-src-container">

<pre class="src src-c++"><span style="color: #98fb98;">int</span> <span style="color: #87cefa;">maxSubSum2</span>(<span style="color: #00ffff;">const</span> <span style="color: #98fb98;">vector</span>&lt;<span style="color: #98fb98;">int</span>&gt; &amp;<span style="color: #eedd82;">a</span>)
{
  <span style="color: #98fb98;">int</span> <span style="color: #eedd82;">maxSum</span> = 0;
  <span style="color: #00ffff;">for</span>(<span style="color: #98fb98;">int</span> <span style="color: #eedd82;">i</span> = 0; i &lt; a.size(); i++){
    <span style="color: #98fb98;">int</span> <span style="color: #eedd82;">thisSum</span> = 0;
    <span style="color: #00ffff;">for</span>(<span style="color: #98fb98;">int</span> <span style="color: #eedd82;">j</span> = i; j &lt; a.size(); j++){
      thisSum += a[j];
      <span style="color: #00ffff;">if</span>(maxSum &lt; thisSum)
        maxSum = thisSum;
    }
  }
  <span style="color: #00ffff;">return</span> maxSum;
}
</pre>
</div>
<ul class="org-ul">
<li>Algorithm 3 (O(Nlog(N)))
</li>
</ul>
<div class="org-src-container">

<pre class="src src-c++"><span style="color: #98fb98;">int</span> <span style="color: #87cefa;">max3</span>(<span style="color: #98fb98;">int</span> <span style="color: #eedd82;">a</span>, <span style="color: #98fb98;">int</span> <span style="color: #eedd82;">b</span>, <span style="color: #98fb98;">int</span> <span style="color: #eedd82;">c</span>)
{
  <span style="color: #98fb98;">int</span> <span style="color: #eedd82;">max</span>;
  <span style="color: #00ffff;">if</span>(a &gt; b)
    max = a;
  <span style="color: #00ffff;">else</span>
    max = b;
  <span style="color: #00ffff;">if</span>(max &lt; c)
    max = c;
  <span style="color: #00ffff;">return</span> max;
}

<span style="color: #98fb98;">int</span> <span style="color: #87cefa;">maxSubRec</span>(<span style="color: #00ffff;">const</span> <span style="color: #98fb98;">vector</span>&lt;<span style="color: #98fb98;">int</span>&gt; &amp;<span style="color: #eedd82;">a</span>, <span style="color: #98fb98;">int</span> <span style="color: #eedd82;">left</span>, <span style="color: #98fb98;">int</span> <span style="color: #eedd82;">right</span>)
{
  <span style="color: #00ffff;">if</span>(left == right)
    <span style="color: #00ffff;">if</span>(a[left] &gt; 0)
      <span style="color: #00ffff;">return</span> a[left];
    <span style="color: #00ffff;">else</span>
      <span style="color: #00ffff;">return</span> 0;

  <span style="color: #98fb98;">int</span> <span style="color: #eedd82;">center</span> = (left + right) / 2;

  <span style="color: #98fb98;">int</span> <span style="color: #eedd82;">maxLeftSum</span> = maxSubRec(a, left, center);
  <span style="color: #98fb98;">int</span> <span style="color: #eedd82;">maxRightSum</span> = maxSubRec(a, center + 1, right);

  <span style="color: #98fb98;">int</span> <span style="color: #eedd82;">maxLeftBorderSum</span> = 0, <span style="color: #eedd82;">leftBorderSum</span> = 0;
  <span style="color: #00ffff;">for</span>(<span style="color: #98fb98;">int</span> <span style="color: #eedd82;">i</span> = center; i &gt;= left; i--){
    leftBorderSum += a[i];
    <span style="color: #00ffff;">if</span>(maxLeftBorderSum &lt; leftBorderSum)
      maxLeftBorderSum = leftBorderSum;
  }

  <span style="color: #98fb98;">int</span> <span style="color: #eedd82;">maxRightBorderSum</span> = 0, <span style="color: #eedd82;">rightBorderSum</span> = 0;
  <span style="color: #00ffff;">for</span>(<span style="color: #98fb98;">int</span> <span style="color: #eedd82;">i</span> = center+1; i &lt;= right; i++){
    rightBorderSum += a[i];
    <span style="color: #00ffff;">if</span>(maxRightBorderSum &lt; rightBorderSum)
      maxRightBorderSum = rightBorderSum;
  }

  <span style="color: #00ffff;">return</span> max3(maxLeftSum, maxRightSum, maxLeftBorderSum + maxRightBorderSum);
}


<span style="color: #98fb98;">int</span> <span style="color: #87cefa;">maxSubSum3</span>(<span style="color: #00ffff;">const</span> <span style="color: #98fb98;">vector</span>&lt;<span style="color: #98fb98;">int</span>&gt; &amp;<span style="color: #eedd82;">a</span>)
{
  <span style="color: #00ffff;">return</span> maxSubRec(a, 0, a.size() - 1);
}
</pre>
</div>

<ul class="org-ul">
<li>Algorithm 4 (O(log(N)))
</li>
</ul>
<div class="org-src-container">

<pre class="src src-c++"><span style="color: #98fb98;">int</span> <span style="color: #87cefa;">maxSubSum4</span>(<span style="color: #00ffff;">const</span> <span style="color: #98fb98;">vector</span>&lt;<span style="color: #98fb98;">int</span>&gt; &amp;<span style="color: #eedd82;">a</span>)
{
  <span style="color: #98fb98;">int</span> <span style="color: #eedd82;">maxSum</span> = 0;
  <span style="color: #98fb98;">int</span> <span style="color: #eedd82;">thisSum</span> = 0;
  <span style="color: #00ffff;">for</span>(<span style="color: #98fb98;">int</span> <span style="color: #eedd82;">i</span> = 0; i &lt; a.size(); i++){
    thisSum += a[i];
    <span style="color: #00ffff;">if</span>(thisSum &lt; 0)
      thisSum = 0;
    <span style="color: #00ffff;">if</span>(thisSum &gt; maxSum)
      maxSum = thisSum;

  }
  <span style="color: #00ffff;">return</span> maxSum;
}
</pre>
</div>

<ul class="org-ul">
<li>test<sub>main</sub>.cpp
</li>
</ul>
<div class="org-src-container">

<pre class="src src-c++"><span style="color: #98fb98;">int</span> <span style="color: #87cefa;">main</span>(<span style="color: #98fb98;">int</span>  <span style="color: #eedd82;">argc</span>, <span style="color: #98fb98;">char</span> *<span style="color: #eedd82;">argv</span>[])
{
  <span style="color: #98fb98;">int</span> <span style="color: #eedd82;">array</span>[] = { 4, -3, 5, -2, -1, 2, 6, -1};
  <span style="color: #98fb98;">vector</span>&lt;<span style="color: #98fb98;">int</span>&gt; <span style="color: #eedd82;">a</span>(array, array + <span style="color: #00ffff;">sizeof</span>(array) / <span style="color: #00ffff;">sizeof</span>(<span style="color: #98fb98;">int</span>));

  <span style="color: #98fb98;">int</span> <span style="color: #eedd82;">maxSum</span>;
  maxSum = maxSubSum1(a);
  cout &lt;&lt; <span style="color: #ffa07a;">"maxSum1 = "</span> &lt;&lt; maxSum &lt;&lt; endl;
  maxSum = maxSubSum2(a);
  cout &lt;&lt; <span style="color: #ffa07a;">"maxSum2 = "</span> &lt;&lt; maxSum &lt;&lt; endl;
  maxSum = maxSubSum3(a);
  cout &lt;&lt; <span style="color: #ffa07a;">"maxSum3 = "</span> &lt;&lt; maxSum &lt;&lt; endl;
  maxSum = maxSubSum4(a);
  cout &lt;&lt; <span style="color: #ffa07a;">"maxSum4 = "</span> &lt;&lt; maxSum &lt;&lt; endl;
  <span style="color: #00ffff;">return</span> 0;
}
</pre>
</div>
</div>
</li>

<li><a id="sec-3-2-4" name="sec-3-2-4"></a>2.4.4<br  /><div class="outline-text-4" id="text-3-2-4">
<ul class="org-ul">
<li>Binary Search(O(logN))
</li>
</ul>
<div class="org-src-container">

<pre class="src src-c++"><span style="color: #00ffff;">template</span> &lt;<span style="color: #00ffff;">class</span> <span style="color: #98fb98;">Comparable</span>&gt;
<span style="color: #98fb98;">int</span> <span style="color: #87cefa;">binarySearch</span>(<span style="color: #00ffff;">const</span> <span style="color: #98fb98;">vecotr</span>&lt;<span style="color: #98fb98;">Comparable</span>&gt; &amp;<span style="color: #eedd82;">a</span>, <span style="color: #00ffff;">const</span> <span style="color: #98fb98;">Comparable</span> &amp;<span style="color: #eedd82;">x</span>)
{
  <span style="color: #98fb98;">int</span> <span style="color: #eedd82;">left</span> = 0, <span style="color: #eedd82;">right</span> = a.size() - 1;

  <span style="color: #00ffff;">while</span>(left &lt; right){
    <span style="color: #98fb98;">int</span> <span style="color: #eedd82;">mid</span> = (left + right) / 2;
    <span style="color: #00ffff;">if</span>(a[mid] == x)
      <span style="color: #00ffff;">return</span> mid;
    <span style="color: #00ffff;">else</span> <span style="color: #00ffff;">if</span>(a[mid] &gt; x)
      right = mid - 1;
    <span style="color: #00ffff;">else</span>
      left = mid + 1;
  }
  <span style="color: #00ffff;">return</span> NOT_FOUND; <span style="color: #ff7f24;">// </span><span style="color: #ff7f24;">NOT_FOUND s defined as -1</span>
}
</pre>
</div>
<ul class="org-ul">
<li>Euclid's Algorithm (O(logN))
</li>
</ul>
<div class="org-src-container">

<pre class="src src-c++"><span style="color: #98fb98;">long</span> <span style="color: #87cefa;">gcd</span>(<span style="color: #98fb98;">long</span> <span style="color: #eedd82;">m</span>, <span style="color: #98fb98;">long</span> <span style="color: #eedd82;">n</span>) <span style="color: #ff7f24;">//</span><span style="color: #ff7f24;">assume m &gt; n</span>
{
  <span style="color: #98fb98;">long</span> <span style="color: #eedd82;">rem</span>;
  <span style="color: #00ffff;">while</span>(n =! 0){
    rem = m % n;
    m = n;
    n = rem;}
  <span style="color: #00ffff;">return</span> m;
}
</pre>
</div>

<ul class="org-ul">
<li>Exponentiation
</li>
</ul>
<div class="org-src-container">

<pre class="src src-c++"><span style="color: #98fb98;">long</span> <span style="color: #87cefa;">pow</span>(<span style="color: #98fb98;">ling</span> <span style="color: #eedd82;">x</span>, <span style="color: #98fb98;">int</span> <span style="color: #eedd82;">n</span>)
{
  <span style="color: #00ffff;">if</span>(n = 0)
    <span style="color: #00ffff;">return</span> 1;

  <span style="color: #00ffff;">if</span>(isEven(n))
    <span style="color: #00ffff;">return</span> pow(x * x, n / 2);
  <span style="color: #00ffff;">else</span>
    <span style="color: #00ffff;">return</span>  pow(x * x, n / 2) * x;
}
</pre>
</div>
</div>
</li></ul>
</div>

<div id="outline-container-sec-3-3" class="outline-3">
<h3 id="sec-3-3">Chapter 3: Lists, Stacks, and Queues</h3>
</div>




<div id="outline-container-sec-3-4" class="outline-3">
<h3 id="sec-3-4">Chapter 4: Trees</h3>
</div>
<div id="outline-container-sec-3-5" class="outline-3">
<h3 id="sec-3-5">CHAPTER 5: HASHING</h3>
</div>
<div id="outline-container-sec-3-6" class="outline-3">
<h3 id="sec-3-6">CHAPTER 6: PRIORITY QUEUES (HEAPS)</h3>
</div>
<div id="outline-container-sec-3-7" class="outline-3">
<h3 id="sec-3-7">CHAPTER 7: SORTING</h3>
</div>
<div id="outline-container-sec-3-8" class="outline-3">
<h3 id="sec-3-8">CHAPTER 8: THE DISJOINT SET ADT</h3>
</div>
<div id="outline-container-sec-3-9" class="outline-3">
<h3 id="sec-3-9">CHAPTER 9: GRAPH ALGORITHMS</h3>
</div>
<div id="outline-container-sec-3-10" class="outline-3">
<h3 id="sec-3-10">CHAPTER 10: ALGORITHM DESIGN TECHNIQUES</h3>
</div>
<div id="outline-container-sec-3-11" class="outline-3">
<h3 id="sec-3-11">CHAPTER 11: AMORTIZED ANALYSIS</h3>
</div>
</div>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" name="fn.1" class="footnum" href="#fnr.1">1</a></sup> <p>DEFINITION NOT FOUND.</p></div>

<div class="footdef"><sup><a id="fn.2" name="fn.2" class="footnum" href="#fnr.2">2</a></sup> <p>DEFINITION NOT FOUND.</p></div>

<div class="footdef"><sup><a id="fn.3" name="fn.3" class="footnum" href="#fnr.3">3</a></sup> <p>DEFINITION NOT FOUND.</p></div>

<div class="footdef"><sup><a id="fn.4" name="fn.4" class="footnum" href="#fnr.4">4</a></sup> <p>DEFINITION NOT FOUND.</p></div>

<div class="footdef"><sup><a id="fn.5" name="fn.5" class="footnum" href="#fnr.5">5</a></sup> <p>DEFINITION NOT FOUND.</p></div>

<div class="footdef"><sup><a id="fn.6" name="fn.6" class="footnum" href="#fnr.6">6</a></sup> <p>DEFINITION NOT FOUND.</p></div>

<div class="footdef"><sup><a id="fn.7" name="fn.7" class="footnum" href="#fnr.7">7</a></sup> <p>DEFINITION NOT FOUND.</p></div>

<div class="footdef"><sup><a id="fn.8" name="fn.8" class="footnum" href="#fnr.8">8</a></sup> <p>DEFINITION NOT FOUND.</p></div>

<div class="footdef"><sup><a id="fn.9" name="fn.9" class="footnum" href="#fnr.9">9</a></sup> <p>DEFINITION NOT FOUND.</p></div>

<div class="footdef"><sup><a id="fn.10" name="fn.10" class="footnum" href="#fnr.10">10</a></sup> <p>DEFINITION NOT FOUND.</p></div>

<div class="footdef"><sup><a id="fn.11" name="fn.11" class="footnum" href="#fnr.11">11</a></sup> <p>DEFINITION NOT FOUND.</p></div>

<div class="footdef"><sup><a id="fn.12" name="fn.12" class="footnum" href="#fnr.12">12</a></sup> <p>DEFINITION NOT FOUND.</p></div>

<div class="footdef"><sup><a id="fn.13" name="fn.13" class="footnum" href="#fnr.13">13</a></sup> <p>DEFINITION NOT FOUND.</p></div>

<div class="footdef"><sup><a id="fn.14" name="fn.14" class="footnum" href="#fnr.14">14</a></sup> <p>DEFINITION NOT FOUND.</p></div>

<div class="footdef"><sup><a id="fn.15" name="fn.15" class="footnum" href="#fnr.15">15</a></sup> <p>DEFINITION NOT FOUND.</p></div>

<div class="footdef"><sup><a id="fn.16" name="fn.16" class="footnum" href="#fnr.16">16</a></sup> <p>DEFINITION NOT FOUND.</p></div>

<div class="footdef"><sup><a id="fn.17" name="fn.17" class="footnum" href="#fnr.17">17</a></sup> <p>DEFINITION NOT FOUND.</p></div>

<div class="footdef"><sup><a id="fn.18" name="fn.18" class="footnum" href="#fnr.18">18</a></sup> <p>DEFINITION NOT FOUND.</p></div>

<div class="footdef"><sup><a id="fn.19" name="fn.19" class="footnum" href="#fnr.19">19</a></sup> <p>DEFINITION NOT FOUND.</p></div>

<div class="footdef"><sup><a id="fn.20" name="fn.20" class="footnum" href="#fnr.20">20</a></sup> <p>DEFINITION NOT FOUND.</p></div>

<div class="footdef"><sup><a id="fn.21" name="fn.21" class="footnum" href="#fnr.21">21</a></sup> <p>DEFINITION NOT FOUND.</p></div>

<div class="footdef"><sup><a id="fn.22" name="fn.22" class="footnum" href="#fnr.22">22</a></sup> <p>DEFINITION NOT FOUND.</p></div>

<div class="footdef"><sup><a id="fn.23" name="fn.23" class="footnum" href="#fnr.23">23</a></sup> <p>DEFINITION NOT FOUND.</p></div>

<div class="footdef"><sup><a id="fn.24" name="fn.24" class="footnum" href="#fnr.24">24</a></sup> <p>DEFINITION NOT FOUND.</p></div>

<div class="footdef"><sup><a id="fn.25" name="fn.25" class="footnum" href="#fnr.25">25</a></sup> <p>DEFINITION NOT FOUND.</p></div>

<div class="footdef"><sup><a id="fn.26" name="fn.26" class="footnum" href="#fnr.26">26</a></sup> <p>DEFINITION NOT FOUND.</p></div>

<div class="footdef"><sup><a id="fn.27" name="fn.27" class="footnum" href="#fnr.27">27</a></sup> <p>DEFINITION NOT FOUND.</p></div>

<div class="footdef"><sup><a id="fn.28" name="fn.28" class="footnum" href="#fnr.28">28</a></sup> <p>DEFINITION NOT FOUND.</p></div>

<div class="footdef"><sup><a id="fn.29" name="fn.29" class="footnum" href="#fnr.29">29</a></sup> <p>DEFINITION NOT FOUND.</p></div>

<div class="footdef"><sup><a id="fn.30" name="fn.30" class="footnum" href="#fnr.30">30</a></sup> <p>DEFINITION NOT FOUND.</p></div>

<div class="footdef"><sup><a id="fn.31" name="fn.31" class="footnum" href="#fnr.31">31</a></sup> <p>DEFINITION NOT FOUND.</p></div>

<div class="footdef"><sup><a id="fn.32" name="fn.32" class="footnum" href="#fnr.32">32</a></sup> <p>DEFINITION NOT FOUND.</p></div>

<div class="footdef"><sup><a id="fn.33" name="fn.33" class="footnum" href="#fnr.33">33</a></sup> <p>DEFINITION NOT FOUND.</p></div>

<div class="footdef"><sup><a id="fn.34" name="fn.34" class="footnum" href="#fnr.34">34</a></sup> <p>DEFINITION NOT FOUND.</p></div>

<div class="footdef"><sup><a id="fn.35" name="fn.35" class="footnum" href="#fnr.35">35</a></sup> <p>DEFINITION NOT FOUND.</p></div>

<div class="footdef"><sup><a id="fn.36" name="fn.36" class="footnum" href="#fnr.36">36</a></sup> <p>DEFINITION NOT FOUND.</p></div>

<div class="footdef"><sup><a id="fn.37" name="fn.37" class="footnum" href="#fnr.37">37</a></sup> <p>DEFINITION NOT FOUND.</p></div>

<div class="footdef"><sup><a id="fn.38" name="fn.38" class="footnum" href="#fnr.38">38</a></sup> <p>DEFINITION NOT FOUND.</p></div>

<div class="footdef"><sup><a id="fn.39" name="fn.39" class="footnum" href="#fnr.39">39</a></sup> <p>DEFINITION NOT FOUND.</p></div>

<div class="footdef"><sup><a id="fn.40" name="fn.40" class="footnum" href="#fnr.40">40</a></sup> <p>DEFINITION NOT FOUND.</p></div>


</div>
</div></div>
<div id="postamble" class="status">
<p class="author">Author: Shi Shougang</p>
<p class="date">Created: 2016-05-26 Thu 22:15</p>
<p class="creator"><a href="http://www.gnu.org/software/emacs/">Emacs</a> 24.3.1 (<a href="http://orgmode.org">Org</a> mode 8.2.10)</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
